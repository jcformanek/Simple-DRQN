{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX4OPDiO8iKD"
   },
   "source": [
    "# Simple DRQN implementaion in TF2\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jcformanek/Simple-TF2-DRQN/blob/main/tf2_drqn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACQlX9eGo18g",
    "outputId": "c02adfa0-79c8-4e1c-c354-c055355fda2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting dm-sonnet\n",
      "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 17.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.21.6)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (0.8.10)\n",
      "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (0.1.7)\n",
      "Installing collected packages: dm-sonnet\n",
      "Successfully installed dm-sonnet-2.0.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting trfl\n",
      "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 15.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trfl) (1.1.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from trfl) (1.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trfl) (1.21.6)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from trfl) (0.1.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trfl) (1.15.0)\n",
      "Installing collected packages: trfl\n",
      "Successfully installed trfl-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dm-sonnet\n",
    "!pip install trfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g4JPLsY7ZcRS"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf \n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "import trfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PxuAHyiLpNmN"
   },
   "outputs": [],
   "source": [
    "class MaskedVelocityCartPole:\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"A wrapper for the CartPole environment which masks out the velocity \n",
    "    components of the observations. This makes it neccessary for agents to have\n",
    "    memory in order to solve the task.\"\"\"\n",
    "\n",
    "    self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "  def reset(self):\n",
    "    return self.env.reset() * np.array([1,0,1,0], \"float32\")\n",
    "\n",
    "  def step(self, action):\n",
    "    obs, rew, done, info = self.env.step(action)\n",
    "    return obs * np.array([1,0,1,0], \"float32\"), rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6o3mlO5bbnFL"
   },
   "outputs": [],
   "source": [
    "class SequenceReplayBuffer:\n",
    "\n",
    "  def __init__(self, obs_size, max_size=2000, batch_size=32, max_sequence_len=20):\n",
    "    \"\"\"Replay buffer that stores sequences rather than just transitions.\"\"\"\n",
    "    \n",
    "    self.obs = np.zeros((max_size, max_sequence_len, obs_size), \"float32\")\n",
    "    self.rewards = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "    self.act = np.zeros((max_size, max_sequence_len), \"int64\")\n",
    "    self.dones = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "    self.zero_mask = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "\n",
    "    # Store prev obs\n",
    "    self.prev_obs = None\n",
    "\n",
    "    # Counters\n",
    "    self.t = 0\n",
    "    self.counter = 0\n",
    "\n",
    "    # Sizes\n",
    "    self.max_size = max_size\n",
    "    self.batch_size = batch_size\n",
    "    self.max_sequence_len = max_sequence_len\n",
    "\n",
    "  def push_first(self, first_obs):\n",
    "    self.prev_obs = first_obs\n",
    "\n",
    "  def push(self, next_obs, action, reward, done):\n",
    "    idx = self.counter % self.max_size\n",
    "    self.obs[idx, self.t] = self.prev_obs\n",
    "    self.act[idx, self.t] = action\n",
    "    self.rewards[idx, self.t] = reward\n",
    "    self.dones[idx, self.t] = done\n",
    "    self.zero_mask[idx, self.t] = 1\n",
    "\n",
    "    self.t += 1\n",
    "    self.prev_obs = next_obs\n",
    "\n",
    "    # End of the episode\n",
    "    if done and self.t < self.max_sequence_len:\n",
    "        self.zero_mask[idx, self.t:] *= 0.0\n",
    "        self.t = self.max_sequence_len\n",
    "\n",
    "    # Move to next sequence\n",
    "    if self.t >= self.max_sequence_len:\n",
    "        self.counter += 1\n",
    "        self.zero_mask[self.counter % self.max_size, :] *= 0.0\n",
    "        self.t = 0\n",
    "\n",
    "  def is_ready(self):\n",
    "    return self.counter >= self.batch_size\n",
    "\n",
    "  def sample(self):\n",
    "    max_idx = min(self.counter, self.max_size)\n",
    "    idxs = np.random.randint(0, max_idx, self.batch_size)\n",
    "\n",
    "    obs_batch = tf.convert_to_tensor(self.obs[idxs])\n",
    "    act_batch = tf.convert_to_tensor(self.act[idxs])\n",
    "    rew_batch = tf.convert_to_tensor(self.rewards[idxs])\n",
    "    done_batch = tf.convert_to_tensor(self.dones[idxs])\n",
    "    zero_mask_batch = tf.convert_to_tensor(self.zero_mask[idxs])\n",
    "\n",
    "    return obs_batch, act_batch, rew_batch, done_batch, zero_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9c5jQUQ-jrKi"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "  def __init__(self, obs_size, num_actions, lr=5e-4, gamma=0.99, target_update=200,\n",
    "    eps_min=0.05, eps_decay_steps=20_000):\n",
    "    \n",
    "    # Parameters\n",
    "    self.obs_size = obs_size\n",
    "    self.num_actions = num_actions\n",
    "    self.gamma = gamma\n",
    "    self.target_update = target_update\n",
    "    self.eps_decay_steps = eps_decay_steps\n",
    "    self.eps_min = eps_min\n",
    "\n",
    "    # Optimiser\n",
    "    self.optimizer = snt.optimizers.Adam(lr)\n",
    "\n",
    "    # Networks\n",
    "    self.q_net = snt.DeepRNN([\n",
    "      snt.Linear(20),\n",
    "      tf.nn.relu,\n",
    "      snt.LSTM(20),\n",
    "      snt.Linear(num_actions)\n",
    "    ])\n",
    "    self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "    # Initialise network variables\n",
    "    dummy_obs = tf.zeros((1,obs_size), \"float32\")\n",
    "    self.q_net(dummy_obs, self.q_net.initial_state(1))\n",
    "    self.target_q_net(dummy_obs, self.target_q_net.initial_state(1))\n",
    "\n",
    "    # Counters\n",
    "    self.learn_step = tf.Variable(0, dtype=\"float32\")\n",
    "    self.act_step = tf.Variable(0, dtype=\"float32\")\n",
    "\n",
    "  def reset(self):\n",
    "    self.hidden_state = self.q_net.initial_state(1)\n",
    "\n",
    "  @tf.function\n",
    "  def q_learning(self, obs, act, rew, done, mask):\n",
    "    B, T = act.shape[:2] # get dims\n",
    "\n",
    "    obs = tf.transpose(obs, perm=[1,0,2]) # make time major for sonnet\n",
    "\n",
    "    # Unroll target network\n",
    "    initial_hidden_state = self.target_q_net.initial_state(B)\n",
    "    target_q_values, _ = snt.static_unroll(self.target_q_net, obs, initial_hidden_state)\n",
    "    target_q_values = tf.transpose(target_q_values, perm=[1,0,2]) # make batch major again\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      # Unroll online network\n",
    "      initial_hidden_state = self.q_net.initial_state(B)\n",
    "      q_values, _ = snt.static_unroll(self.q_net, obs, initial_hidden_state)\n",
    "      q_values = tf.transpose(q_values, perm=[1,0,2]) # make batch major again\n",
    "\n",
    "      # Extract q_value of chose action\n",
    "      act_q_value = trfl.batched_index(q_values, act)\n",
    "\n",
    "      # Double Q-learning\n",
    "      target_act = tf.argmax(q_values, axis=-1)\n",
    "      target_q_value = trfl.batched_index(target_q_values, target_act)\n",
    "\n",
    "      # Extract the timesteps we want\n",
    "      act_q_value = act_q_value[:,:-1] # chop off last timestep\n",
    "      target_q_value = target_q_value[:,1:] # chop of first timestep\n",
    "      act = act[:,:-1]\n",
    "      rew = rew[:,:-1]\n",
    "      done = done[:,:-1]\n",
    "      mask = mask[:,:-1]\n",
    "\n",
    "      # Bellman target\n",
    "      target = tf.stop_gradient(rew + self.gamma * (1-done) * target_q_value)\n",
    "\n",
    "      # Squared TD Error\n",
    "      squared_td_error = (target - act_q_value) ** 2\n",
    "\n",
    "      # Masked mean loss\n",
    "      loss = tf.reduce_sum(squared_td_error * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "    # Comput and apply gradients\n",
    "    variables = self.q_net.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    self.optimizer.apply(gradients, variables)\n",
    "\n",
    "    # Update target network\n",
    "    if self.learn_step % self.target_update == 0:\n",
    "      online_variables = self.q_net.variables\n",
    "      target_variables = self.target_q_net.variables\n",
    "      for src, dest in zip(online_variables, target_variables):\n",
    "        dest.assign(src)\n",
    "\n",
    "    # Increment counter\n",
    "    self.learn_step.assign_add(1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  @tf.function\n",
    "  def greedy_action_selection(self, obs, hidden_state):\n",
    "    obs = tf.expand_dims(obs, axis=0) # dummy batch dim\n",
    "\n",
    "    q_values, hidden_state = self.q_net(obs, hidden_state)\n",
    "\n",
    "    # Greedy action\n",
    "    action = tf.argmax(q_values, axis=-1)\n",
    "\n",
    "    return action, hidden_state\n",
    "\n",
    "  def epsilon_greedy_action_selection(self, obs, evaluation=False):\n",
    "\n",
    "    # Greedy action\n",
    "    tf_obs = tf.convert_to_tensor(obs, \"float32\")\n",
    "    action, self.hidden_state = self.greedy_action_selection(tf_obs, self.hidden_state)\n",
    "    action = action.numpy()[0]\n",
    "\n",
    "    # Get Epsilon\n",
    "    epsilon = 1.0 - self.act_step / self.eps_decay_steps\n",
    "    epsilon = self.eps_min if epsilon < self.eps_min else epsilon\n",
    "\n",
    "    # Random action\n",
    "    if np.random.random() < epsilon and not evaluation:\n",
    "      action = np.random.randint(0, self.num_actions, size=1)[0]\n",
    "\n",
    "    # Increment counter\n",
    "    self.act_step.assign_add(1)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_N2YKxXfx5n",
    "outputId": "9244eefe-b13e-4945-ce0e-d0830c6dbd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0      Avg. Episode return: 21.0     Epsilon 0.999    Train steps: 0    Timesteps 21\n",
      "Episode 100      Avg. Episode return: 18.66     Epsilon 0.897    Train steps: 1118    Timesteps 2060\n",
      "Episode 200      Avg. Episode return: 21.16     Epsilon 0.798    Train steps: 3096    Timesteps 4038\n",
      "Episode 300      Avg. Episode return: 23.12     Epsilon 0.678    Train steps: 5497    Timesteps 6439\n",
      "Episode 400      Avg. Episode return: 29.8     Epsilon 0.545    Train steps: 8162    Timesteps 9104\n",
      "Episode 500      Avg. Episode return: 26.38     Epsilon 0.404    Train steps: 10975    Timesteps 11917\n",
      "Episode 600      Avg. Episode return: 103.04     Epsilon 0.058    Train steps: 17894    Timesteps 18836\n",
      "Episode 700      Avg. Episode return: 369.14     Epsilon 0.05    Train steps: 43676    Timesteps 44618\n",
      "Episode 800      Avg. Episode return: 309.72     Epsilon 0.05    Train steps: 78278    Timesteps 79220\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10_000\n",
    "batch_size = 64\n",
    "\n",
    "env = MaskedVelocityCartPole()\n",
    "\n",
    "agent = Agent(4, 2)\n",
    "\n",
    "mem = SequenceReplayBuffer(4, max_size=10_000, batch_size=batch_size)\n",
    "\n",
    "ep_returns = []\n",
    "for e in range(num_episodes):\n",
    "\n",
    "  obs = env.reset()\n",
    "  agent.reset()\n",
    "  mem.push_first(obs)\n",
    "  \n",
    "  ep_return = 0\n",
    "  done = False\n",
    "  while not done:\n",
    "    # Step actor\n",
    "    action = agent.epsilon_greedy_action_selection(obs)\n",
    "\n",
    "    # Step environment\n",
    "    obs, rew, done, _ = env.step(action)\n",
    "\n",
    "    # Push transition to memory\n",
    "    mem.push(obs, action, rew, done)\n",
    "\n",
    "    # Add reward to return\n",
    "    ep_return += rew\n",
    "\n",
    "    # Do some learning\n",
    "    if mem.is_ready():\n",
    "      # Sample memory\n",
    "      train_obs, train_act, train_rew, train_done, train_mask = mem.sample()\n",
    "\n",
    "      agent.q_learning(train_obs, train_act, train_rew, train_done, train_mask)\n",
    "      \n",
    "  # Add ep_return to list\n",
    "  ep_returns.append(ep_return)\n",
    "\n",
    "  # Logging\n",
    "  if e % 100 == 0:\n",
    "    epsilon = max(agent.eps_min, 1.0 - agent.act_step / agent.eps_decay_steps)\n",
    "    print(\"Episode\", e, \"     Avg. Episode return:\", np.mean(ep_returns[-50:]), \"    Epsilon\", round(float(epsilon), 3), \"   Train steps:\", int(agent.learn_step), \"   Timesteps\", int(agent.act_step))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUZIa_4c1ILn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RDQN.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
