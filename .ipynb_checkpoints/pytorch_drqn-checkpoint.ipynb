{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eaac0db-eb4d-4f7c-9aca-7c3e855c3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64b7b6b-8d4f-41e1-8db1-2ac3b492a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceReplayBuffer:\n",
    "    def __init__(self, obs_size, max_size=2000, batch_size=32, max_sequence_len=20):\n",
    "        \"\"\"Replay buffer that stores sequences rather than just transitions.\"\"\"\n",
    "        self.obs = np.zeros((max_size, max_sequence_len, obs_size), \"float32\")\n",
    "        self.rewards = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "        self.act = np.zeros((max_size, max_sequence_len), \"int64\")\n",
    "        self.dones = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "        self.zero_mask = np.zeros((max_size, max_sequence_len), \"float32\")\n",
    "\n",
    "        # Store prev obs\n",
    "        self.prev_obs = None\n",
    "\n",
    "        # Counters\n",
    "        self.t = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        # Sizes\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "\n",
    "    def push_first(self, first_obs):\n",
    "        self.prev_obs = first_obs\n",
    "\n",
    "    def push(self, next_obs, action, reward, done):\n",
    "        idx = self.counter % self.max_size\n",
    "        self.obs[idx, self.t] = self.prev_obs\n",
    "        self.act[idx, self.t] = action\n",
    "        self.rewards[idx, self.t] = reward\n",
    "        self.dones[idx, self.t] = done\n",
    "        self.zero_mask[idx, self.t] = 1\n",
    "\n",
    "        # Step forward in time\n",
    "        self.t += 1\n",
    "        self.prev_obs = next_obs\n",
    "\n",
    "        # End of the episode\n",
    "        if done and self.t < self.max_sequence_len:\n",
    "            self.zero_mask[idx, self.t:] *= 0.0\n",
    "            self.t = self.max_sequence_len\n",
    "\n",
    "        # Move to next sequence\n",
    "        if self.t >= self.max_sequence_len:\n",
    "            self.counter += 1\n",
    "            self.zero_mask[self.counter % self.max_size, :] *= 0.0\n",
    "            self.t = 0\n",
    "\n",
    "    def is_ready(self):\n",
    "        return self.counter >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        max_idx = min(self.counter, self.max_size)\n",
    "        idxs = np.random.randint(0, max_idx, self.batch_size)\n",
    "\n",
    "        obs_batch = torch.from_numpy(self.obs[idxs])\n",
    "        act_batch = torch.from_numpy(self.act[idxs])\n",
    "        rew_batch = torch.from_numpy(self.rewards[idxs])\n",
    "        done_batch = torch.from_numpy(self.dones[idxs])\n",
    "        zero_mask_batch = torch.from_numpy(self.zero_mask[idxs])\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, done_batch, zero_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c621d7f-3a00-4a20-9c0d-3c1fb773d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_lstm, out_features):\n",
    "        super(DQNRNN, self).__init__()\n",
    "        self.h_dim = hidden_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_lstm, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, obs, hidden_state, targ_device=torch.device(\"cpu\")):\n",
    "        hn, cn = hidden_state\n",
    "        x = F.gelu(self.fc1(obs))\n",
    "        x, (hn, cn) = self.lstm(x, (hn, cn))\n",
    "        return self.head(x), (hn, cn)\n",
    "\n",
    "    def reset_hidden_state(self, batch_size):\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        if batch_size == 1:\n",
    "            size = (1, self.h_dim)\n",
    "        else:\n",
    "            size = (1, batch_size, self.h_dim)\n",
    "        h0 = torch.zeros(size, dtype=torch.float32)\n",
    "        c0 = torch.zeros(size, dtype=torch.float32)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac2774ac-1f45-4cb0-a840-83dc090e9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            obs_size,\n",
    "            num_actions,\n",
    "            num_rnn=1,\n",
    "            rnn_hidden_dim=8,\n",
    "            lr=5e-4,\n",
    "            gamma=0.99,\n",
    "            target_update=200,\n",
    "            eps_min=0.05,\n",
    "            eps_decay_steps=20_000\n",
    "    ):\n",
    "        # Parameters\n",
    "        self.obs_size = obs_size\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        self.eps_decay_steps = eps_decay_steps\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        # Networks\n",
    "        network_params = {\n",
    "            'input_size': obs_size,\n",
    "            'hidden_size': rnn_hidden_dim,\n",
    "            'num_lstm': num_rnn,\n",
    "            'out_features': num_actions,\n",
    "        }\n",
    "        self.q_net = DQNRNN(**network_params)\n",
    "        self.target_q_net = DQNRNN(**network_params)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_q_net.eval()\n",
    "\n",
    "        # Optimiser\n",
    "        self.optimizer = Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Counters\n",
    "        self.learn_step = 0\n",
    "        self.act_step = 0\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        self.hidden_state = self.q_net.reset_hidden_state(batch_size)\n",
    "\n",
    "    def q_learning(self, obs, act, rew, done, mask):\n",
    "        B, T = act.shape  # (batch_size, sequence_length)\n",
    "        \n",
    "        # Unroll online network\n",
    "        hidden_state = self.q_net.reset_hidden_state(B)\n",
    "        q_values, _ = self.q_net.forward(obs, hidden_state)\n",
    "        \n",
    "        # Extract q_value of chose action\n",
    "        act_q_value = q_values.gather(2, act.unsqueeze(2)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Unroll target network\n",
    "            hidden_state = self.target_q_net.reset_hidden_state(B)\n",
    "            target_q_values, _ = self.target_q_net.forward(obs, hidden_state)\n",
    "\n",
    "            # Double Q-learning\n",
    "            target_act = q_values.max(2)[1]  # (batch_size, seq_len, actions)\n",
    "            target_q_value = target_q_values.gather(2, target_act.unsqueeze(2)).squeeze().detach()\n",
    "\n",
    "        # Extract the timesteps we want\n",
    "        act_q_value = act_q_value[:, :T-1]  # chop off last timestep\n",
    "        act = act[:, :-1]\n",
    "        rew = rew[:, :-1]\n",
    "        done = done[:, :-1]\n",
    "        mask = mask[:, :-1]\n",
    "        target_q_value = target_q_value[:, 1:]  # chop of first timestep\n",
    "\n",
    "        # Bellman target\n",
    "        bellman_target = rew + self.gamma * (1-done) * target_q_value\n",
    "\n",
    "        # Compute masked loss\n",
    "        squared_td_error = (bellman_target - act_q_value) ** 2\n",
    "        loss = torch.sum(squared_td_error * mask) / torch.sum(mask)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for param in Q.parameters():\n",
    "        #     param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target net\n",
    "        if self.learn_step % self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # Increment counter\n",
    "        self.learn_step += 1\n",
    "        \n",
    "        return loss.detach().item()\n",
    "\n",
    "    def select_action(self, obs, greedy=False, evaluation=False):\n",
    "        # Have to pass through network to update hidden state\n",
    "        with torch.no_grad():\n",
    "            obs = torch.from_numpy(obs).unsqueeze(0)\n",
    "            q_values, self.hidden_state = self.q_net(obs, self.hidden_state)\n",
    "\n",
    "        # Get Epsilon\n",
    "        epsilon = max(self.eps_min, 1.0 - self.act_step / self.eps_decay_steps)\n",
    "        \n",
    "        # Greedy action selection vs exploration\n",
    "        if random.random() > epsilon or greedy:\n",
    "            a = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            a = np.random.randint(0, self.num_actions, size=1)[0]\n",
    "\n",
    "        # Increment counter\n",
    "        self.act_step += 1\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f0ce35-ec6f-42cf-af2e-1b1bd4ea973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedVelocityCartPole:\n",
    "    def __init__(self):\n",
    "        \"\"\"A wrapper for the CartPole environment which masks out the velocity\n",
    "        components of the observations. This makes it neccessary for agents to have\n",
    "        memory in order to solve the task.\"\"\"\n",
    "\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset() * np.array([1, 0, 1, 0], \"float32\")\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        return obs * np.array([1, 0, 1, 0], \"float32\"), rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c7ae9-aa54-4c6b-a91d-e81a7c1f7948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode\t0\tAvg. Episode return:\t14.0\tEpsilon\t0.999\tTrain steps:\t0\tTimesteps\t14\n",
      "Episode\t100\tAvg. Episode return:\t23.62\tEpsilon\t0.892\tTrain steps:\t1294\tTimesteps\t2169\n",
      "Episode\t200\tAvg. Episode return:\t24.96\tEpsilon\t0.768\tTrain steps:\t3770\tTimesteps\t4645\n",
      "Episode\t300\tAvg. Episode return:\t24.66\tEpsilon\t0.647\tTrain steps:\t6194\tTimesteps\t7069\n",
      "Episode\t400\tAvg. Episode return:\t20.86\tEpsilon\t0.542\tTrain steps:\t8294\tTimesteps\t9169\n",
      "Episode\t500\tAvg. Episode return:\t36.66\tEpsilon\t0.399\tTrain steps:\t11147\tTimesteps\t12022\n",
      "Episode\t600\tAvg. Episode return:\t136.94\tEpsilon\t0.05\tTrain steps:\t21600\tTimesteps\t22475\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 10_000\n",
    "batch_size = 64\n",
    "seed = 0\n",
    "\n",
    "# for reproducability\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed) \n",
    "\n",
    "# Initialise env, agent, memory\n",
    "env = MaskedVelocityCartPole()\n",
    "agent = Agent(obs_size=4, num_actions=2, rnn_hidden_dim=20)\n",
    "mem = SequenceReplayBuffer(obs_size=4, max_size=10_000, batch_size=batch_size)\n",
    "\n",
    "ep_returns = []\n",
    "for e in range(num_episodes):\n",
    "    obs, done = env.reset(), False\n",
    "    agent.reset(1)\n",
    "    mem.push_first(obs)\n",
    "    \n",
    "    ep_return = 0\n",
    "    while not done:\n",
    "        # Step actor\n",
    "        action = agent.select_action(obs)\n",
    "\n",
    "        # Step environment\n",
    "        obs, rew, done, _ = env.step(action)\n",
    "\n",
    "        # Push transition to memory\n",
    "        mem.push(obs, action, rew, done)\n",
    "\n",
    "        # Add reward to return\n",
    "        ep_return += rew\n",
    "\n",
    "        # Do some learning\n",
    "        if mem.is_ready():\n",
    "            # Sample memory\n",
    "            train_obs, train_act, train_rew, train_done, train_mask = mem.sample()\n",
    "            agent.q_learning(train_obs, train_act, train_rew, train_done, train_mask)\n",
    "\n",
    "    # Add ep_return to list\n",
    "    ep_returns.append(ep_return)\n",
    "\n",
    "    # Logging\n",
    "    if e % 100 == 0:\n",
    "        epsilon = max(agent.eps_min, 1.0 - agent.act_step / agent.eps_decay_steps)\n",
    "        print(\n",
    "            \"Episode\", e,\n",
    "            \"Avg. Episode return:\", np.mean(ep_returns[-50:]),\n",
    "            \"Epsilon\", round(float(epsilon), 3),\n",
    "            \"Train steps:\", int(agent.learn_step),\n",
    "            \"Timesteps\", int(agent.act_step),\n",
    "            sep='\\t'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ccb75-7784-416e-92b7-743f4f1f5663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
